{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cf0a04-9940-4086-badb-256169cc4fdb",
   "metadata": {},
   "source": [
    "# üé® DreamScape AI ‚Äî Multimodal Generation Notebook\n",
    "\n",
    "This notebook generates **dream-inspired multimodal outputs** ‚Äî combining *text, audio, and visuals* using large generative models.  \n",
    "It integrates:\n",
    "- Stable Diffusion (SD-Turbo) for **illustrations**\n",
    "- Faster-Whisper for **speech transcription**\n",
    "- MusicGen (via ü§ó Transformers) for **ambient sound**\n",
    "- Detoxify for **safety filtering**\n",
    "- NetworkX for **motif graphing**\n",
    "\n",
    "All assets and metadata are saved under `results/` for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f069906f-31b9-457e-aa16-357466d55526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Environment Setup ---\n",
    "%pip -q install --upgrade diffusers accelerate pillow gradio soundfile librosa faster-whisper detoxify torch torchvision torchaudio transformers networkx matplotlib scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d6885-e79d-4e94-963c-0e36720cc5e5",
   "metadata": {},
   "source": [
    "##  Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3df58f-caf3-48ef-96d8-00a1b543f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, io, math, json, random, warnings, tempfile, re, itertools\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from detoxify import Detoxify\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "OUT_DIR = Path(\"results\")\n",
    "OUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc13ae8-757b-43c8-8fe1-db7c3e8db10c",
   "metadata": {},
   "source": [
    "##  Load Stable Diffusion Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114d32e0-ccb2-48d4-b485-2bd2edd35438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'dtype': torch.float32} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630935df04a24bb78dce2f58cd538335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text‚Üíimage: stabilityai/sd-turbo on mps (dtype=torch.float32); safety=ON\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "SafetyChecker = None\n",
    "ClipProcessor = None\n",
    "try:\n",
    "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker as SafetyChecker\n",
    "except Exception:\n",
    "    try:\n",
    "        from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker as SafetyChecker\n",
    "    except Exception:\n",
    "        SafetyChecker = None\n",
    "\n",
    "try:\n",
    "    from transformers import CLIPImageProcessor as ClipProcessor\n",
    "except Exception:\n",
    "    try:\n",
    "        from transformers import AutoImageProcessor as ClipProcessor\n",
    "    except Exception:\n",
    "        ClipProcessor = None\n",
    "\n",
    "TXT2IMG_ID = \"stabilityai/sd-turbo\"\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "safety_checker = None\n",
    "feature_extractor = None\n",
    "if SafetyChecker and ClipProcessor:\n",
    "    try:\n",
    "        safety_checker = SafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        feature_extractor = ClipProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Safety checker unavailable: {e}\")\n",
    "\n",
    "pipe_kwargs = dict(dtype=dtype)\n",
    "if safety_checker and feature_extractor:\n",
    "    pipe_kwargs[\"safety_checker\"] = safety_checker\n",
    "    pipe_kwargs[\"feature_extractor\"] = feature_extractor\n",
    "\n",
    "TXT2IMG_PIPE = AutoPipelineForText2Image.from_pretrained(TXT2IMG_ID, **pipe_kwargs).to(device)\n",
    "try:\n",
    "    TXT2IMG_PIPE.enable_attention_slicing()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"Loaded text‚Üíimage: {TXT2IMG_ID} on {device} (dtype={dtype}); \"\n",
    "      f\"safety={'ON' if safety_checker else 'OFF (blur fallback)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b86e1-2722-4082-ae2b-d2c9efc5c153",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e6bcd1-fd54-460f-bb77-b9a63c671d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int | None):\n",
    "    if seed is None:\n",
    "        return\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def sanitize_prompt(prompt: str):\n",
    "    \"\"\"Filter toxic or unsafe language before generation.\"\"\"\n",
    "    try:\n",
    "        scores = Detoxify('original').predict(prompt)\n",
    "        tox_score = float(scores.get('toxicity', 0.0))\n",
    "    except Exception:\n",
    "        tox_score = 0.0\n",
    "    cleaned = prompt.strip()\n",
    "    softened = False\n",
    "    if tox_score > 0.5:\n",
    "        cleaned = \"A calm, imaginative reinterpretation of \" + prompt\n",
    "        softened = True\n",
    "    return cleaned, tox_score, softened\n",
    "\n",
    "def save_image(img: Image.Image, path: Path) -> Path:\n",
    "    img.save(path, format=\"PNG\")\n",
    "    return path\n",
    "\n",
    "def now_stamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3036c7-9415-48d3-9e5d-9243488b9cf5",
   "metadata": {},
   "source": [
    "## Audio Generation (MusicGen via ü§ó Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d10a767-a9dc-4db5-bbad-ae5d369f3f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versions => transformers 4.57.1 | torch 2.9.0 | scipy 1.16.3 | hf-hub 0.36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Loaded MusicGen via pipeline: facebook/musicgen-small\n",
      "üîä AUDIO_ENGINE active: MusicGen/Transformers(facebook/musicgen-small)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Audio Generation (MusicGen via ü§ó Transformers, or Ambient Fallback) ---\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import pipeline, AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "MUSICGEN_MODEL_ID = \"facebook/musicgen-small\"   # you can try \"facebook/musicgen-medium\" later\n",
    "# Use CPU for maximum compatibility (CUDA if you have NVIDIA, but CPU is fine here)\n",
    "TTA_DEVICE = -1\n",
    "\n",
    "TTA_PIPE = None\n",
    "TTA_DIRECT = None\n",
    "AUDIO_ENGINE = \"AmbientFallback\"\n",
    "\n",
    "def _print_versions():\n",
    "    import transformers, torch, scipy, huggingface_hub\n",
    "    print(\n",
    "        \"versions =>\",\n",
    "        \"transformers\", transformers.__version__,\n",
    "        \"| torch\", torch.__version__,\n",
    "        \"| scipy\", scipy.__version__,\n",
    "        \"| hf-hub\", huggingface_hub.__version__,\n",
    "    )\n",
    "\n",
    "_print_versions()\n",
    "\n",
    "# 1) Try the Transformers pipeline(\"text-to-audio\") first\n",
    "try:\n",
    "    TTA_PIPE = pipeline(\n",
    "        task=\"text-to-audio\",\n",
    "        model=MUSICGEN_MODEL_ID,\n",
    "        device=TTA_DEVICE,  # -1=CPU\n",
    "    )\n",
    "    AUDIO_ENGINE = f\"MusicGen/Transformers({MUSICGEN_MODEL_ID})\"\n",
    "    print(f\"üéµ Loaded MusicGen via pipeline: {MUSICGEN_MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è pipeline(text-to-audio) failed:\", e)\n",
    "    TTA_PIPE = None\n",
    "\n",
    "# 2) If pipeline failed, try direct model+processor\n",
    "if TTA_PIPE is None:\n",
    "    try:\n",
    "        proc = AutoProcessor.from_pretrained(MUSICGEN_MODEL_ID)\n",
    "        mdl  = MusicgenForConditionalGeneration.from_pretrained(MUSICGEN_MODEL_ID)\n",
    "        mdl = mdl.to(\"cpu\")\n",
    "        TTA_DIRECT = (proc, mdl)\n",
    "        AUDIO_ENGINE = f\"MusicGen/Direct({MUSICGEN_MODEL_ID})\"\n",
    "        print(f\"üéµ Loaded MusicGen via direct model: {MUSICGEN_MODEL_ID}\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Direct MusicGen load failed:\", e)\n",
    "        TTA_DIRECT = None\n",
    "        AUDIO_ENGINE = \"AmbientFallback\"\n",
    "        print(\"‚û°Ô∏è Will use ambient fallback for audio.\")\n",
    "\n",
    "def text_to_audio(prompt: str, path: Path, seconds: int = 8) -> Path:\n",
    "    \"\"\"\n",
    "    Prefer MusicGen (pipeline). If unavailable, try direct model.\n",
    "    If both fail, write a quiet sine tone (ambient fallback).\n",
    "    \"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # A) Pipeline route\n",
    "    if TTA_PIPE is not None:\n",
    "        try:\n",
    "            # ~50 tokens/sec is a good rough target\n",
    "            result = TTA_PIPE(\n",
    "                prompt,\n",
    "                forward_params={\"do_sample\": True, \"max_new_tokens\": int(seconds * 50)}\n",
    "            )\n",
    "            sr = int(result[\"sampling_rate\"])\n",
    "            audio = result[\"audio\"]  # float32 mono [-1,1]\n",
    "            wavfile.write(path, rate=sr, data=(audio * 32767).astype(np.int16))\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è MusicGen pipeline failed, trying direct:\", e)\n",
    "\n",
    "    # B) Direct route\n",
    "    if TTA_DIRECT is not None:\n",
    "        try:\n",
    "            proc, mdl = TTA_DIRECT\n",
    "            inputs = proc(text=[prompt], padding=True, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                # seconds * 50 tokens ‚âà rough duration control\n",
    "                audio_values = mdl.generate(**inputs, max_new_tokens=int(seconds * 50))\n",
    "            sr = mdl.config.audio_encoder.sampling_rate\n",
    "            audio = audio_values[0, 0].cpu().numpy()  # (samples,)\n",
    "            wavfile.write(path, rate=sr, data=(audio * 32767).astype(np.int16))\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è MusicGen direct failed, falling back:\", e)\n",
    "\n",
    "    # C) Fallback (quiet sine)\n",
    "    sr = 22050\n",
    "    t = np.linspace(0, 5, int(5 * sr), endpoint=False)\n",
    "    tone = 0.02 * np.sin(2 * np.pi * 220 * t)\n",
    "    wavfile.write(path, sr, (tone * 32767).astype(np.int16))\n",
    "    return path\n",
    "\n",
    "print(f\"üîä AUDIO_ENGINE active: {AUDIO_ENGINE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40544da-6562-42ed-a041-9e7128514b2b",
   "metadata": {},
   "source": [
    "## Text ‚Üí Image and Multimodal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75a71be-8a34-4faf-b6d6-896a378e2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image(\n",
    "    prompt: str,\n",
    "    negative: str = \"low quality, blurry, watermark, text, logo\",\n",
    "    steps: int = 4,\n",
    "    guidance: float = 0.0,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    seed: int | None = 1234\n",
    ") -> Image.Image:\n",
    "    set_seed(seed)\n",
    "    try:\n",
    "        out = TXT2IMG_PIPE(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=guidance,\n",
    "            height=height, width=width,\n",
    "        )\n",
    "        img = out.images[0]\n",
    "        flagged = False\n",
    "        if hasattr(out, \"nsfw_content_detected\") and out.nsfw_content_detected:\n",
    "            flagged = bool(out.nsfw_content_detected[0])\n",
    "        if flagged:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=24))\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Generation retry on CPU:\", e)\n",
    "        pipe_cpu = TXT2IMG_PIPE.to(\"cpu\")\n",
    "        out = pipe_cpu(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative,\n",
    "            num_inference_steps=max(steps, 8),\n",
    "            guidance_scale=max(guidance, 1.0),\n",
    "            height=height, width=width,\n",
    "        )\n",
    "        img = out.images[0]\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4debea-d4df-46a7-a2d0-f28c6a95fff5",
   "metadata": {},
   "source": [
    " ## Moodboard + Motif Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db76302f-4063-41b8-8672-253452766fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _square_thumb(im: Image.Image, size=384):\n",
    "    w, h = im.size\n",
    "    s = min(w, h)\n",
    "    im = im.crop(((w-s)//2, (h-s)//2, (w+s)//2, (h+s)//2))\n",
    "    return im.resize((size, size), Image.LANCZOS)\n",
    "\n",
    "def compose_grid(images, cols=3, pad=8, bg=(18,18,18)):\n",
    "    tiles = [_square_thumb(img) for img in images]\n",
    "    w, h = tiles[0].size\n",
    "    rows = math.ceil(len(tiles)/cols)\n",
    "    W, H = cols*w+(cols+1)*pad, rows*h+(rows+1)*pad\n",
    "    canvas = Image.new(\"RGB\", (W,H), bg)\n",
    "    for i, t in enumerate(tiles):\n",
    "        r, c = divmod(i, cols)\n",
    "        canvas.paste(t, (pad+c*(w+pad), pad+r*(h+pad)))\n",
    "    return canvas\n",
    "\n",
    "def text_to_moodboard(text: str, n_images=6, cols=3):\n",
    "    base_prompt, _, _ = sanitize_prompt(text)\n",
    "    styles = [\n",
    "        \"surreal cinematic volumetric light\", \"dreamlike watercolor pastel\",\n",
    "        \"digital art neon synthwave\", \"oil painting baroque lighting\",\n",
    "        \"minimalist muted palette\", \"storybook ink & wash\"\n",
    "    ]\n",
    "    imgs=[]\n",
    "    for i in range(n_images):\n",
    "        styl = styles[i % len(styles)]\n",
    "        prompt_i = f\"{base_prompt}. Style: {styl}\"\n",
    "        img = text_to_image(prompt_i, seed=1234+i)\n",
    "        imgs.append(img)\n",
    "    return compose_grid(imgs, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e49d301-4261-4cf8-b20c-2d5149a4434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeea0ac78a1423fbce1ed16029908cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1c3844019f4a2b99cb48f37707d076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46dec880ae84f8d88ece5a2187fdfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3df9d9dda944806a3b21960a2315b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863467aeb2a84172bb8b5630270500c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c85f8003494b738641a0c7cc16d5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Motif NER: dslim/bert-base-NER loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Smarter Motif Graph (NER-backed with lexicon fallback) ---\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to load a lightweight NER pipeline\n",
    "NER_PIPE = None\n",
    "try:\n",
    "    from transformers import pipeline as hf_pipeline\n",
    "    # CPU is fine; this model is small and downloads once\n",
    "    NER_PIPE = hf_pipeline(\n",
    "        task=\"token-classification\",\n",
    "        model=\"dslim/bert-base-NER\",\n",
    "        aggregation_strategy=\"simple\",\n",
    "        device=-1\n",
    "    )\n",
    "    print(\"üß© Motif NER: dslim/bert-base-NER loaded.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Motif NER unavailable, using lexicon only.\", e)\n",
    "    NER_PIPE = None\n",
    "\n",
    "# Expanded motif lexicon (common dream nouns & symbols)\n",
    "MOTIF_LEXICON = {\n",
    "    \"forest\",\"tree\",\"river\",\"sea\",\"ocean\",\"wave\",\"water\",\"rain\",\"storm\",\"cloud\",\"sky\",\"moon\",\"sun\",\"star\",\"night\",\"light\",\"shadow\",\n",
    "    \"mirror\",\"glass\",\"window\",\"door\",\"stairs\",\"bridge\",\"room\",\"house\",\"city\",\"desert\",\"mountain\",\"valley\",\"garden\",\"flower\",\n",
    "    \"bird\",\"cat\",\"dog\",\"fish\",\"horse\",\"person\",\"child\",\"friend\",\"stranger\",\n",
    "    \"clock\",\"time\",\"book\",\"key\",\"phone\",\"car\",\"train\",\"boat\",\"plane\",\n",
    "    \"color\",\"red\",\"blue\",\"green\",\"gold\",\"violet\",\"purple\",\"black\",\"white\",\n",
    "    \"fire\",\"ice\",\"snow\",\"fog\",\"mist\",\"sand\"\n",
    "}\n",
    "\n",
    "_token = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "def _heuristic_motifs(text: str):\n",
    "    \"\"\"Fallback: pick words that intersect our lexicon.\"\"\"\n",
    "    toks = [w.lower() for w in _token.findall(text)]\n",
    "    return [w for w in toks if w in MOTIF_LEXICON]\n",
    "\n",
    "def _ner_motifs(text: str):\n",
    "    \"\"\"Use NER to extract salient tokens (LOC/ORG/PER/MISC labels).\"\"\"\n",
    "    if NER_PIPE is None:\n",
    "        return []\n",
    "    ents = NER_PIPE(text)\n",
    "    # Keep reasonably short entity strings; split multiword into tokens as motifs too\n",
    "    motifs = []\n",
    "    for e in ents:\n",
    "        span = e.get(\"word\", \"\") or e.get(\"entity_group\", \"\")\n",
    "        span = span.strip()\n",
    "        if not span:\n",
    "            continue\n",
    "        # break multiword spans into individual tokens plus the span itself\n",
    "        parts = [w.lower() for w in _token.findall(span)]\n",
    "        if span and len(span) <= 24:\n",
    "            motifs.append(span.lower())\n",
    "        motifs.extend(parts)\n",
    "    # If NER returned nothing, return empty (caller will fallback/merge)\n",
    "    return [m for m in motifs if len(m) >= 2]\n",
    "\n",
    "def extract_motifs(text: str, top_k: int = 20):\n",
    "    \"\"\"Combine NER + lexicon, deduplicate, keep order of appearance.\"\"\"\n",
    "    ner = _ner_motifs(text)\n",
    "    lex = _heuristic_motifs(text)\n",
    "    combined = []\n",
    "    seen = set()\n",
    "    for w in ner + lex:\n",
    "        if w not in seen:\n",
    "            combined.append(w)\n",
    "            seen.add(w)\n",
    "        if len(combined) >= top_k:\n",
    "            break\n",
    "    return combined\n",
    "\n",
    "def build_motif_graph(text: str, window: int = 4, min_weight: int = 1):\n",
    "    \"\"\"\n",
    "    Build a co-occurrence graph over extracted motifs within a sliding window.\n",
    "    Keeps even single co-occurrences by default to avoid an empty graph.\n",
    "    \"\"\"\n",
    "    motifs = extract_motifs(text, top_k=40)\n",
    "    G = nx.Graph()\n",
    "    # Add nodes even if we end with no edges‚Äîso the graph isn‚Äôt blank.\n",
    "    for m in motifs:\n",
    "        G.add_node(m)\n",
    "\n",
    "    # Co-occurrence edges\n",
    "    for i in range(len(motifs)):\n",
    "        for j in range(i + 1, min(i + window, len(motifs))):\n",
    "            if motifs[i] != motifs[j]:\n",
    "                a, b = sorted((motifs[i], motifs[j]))\n",
    "                w = G.get_edge_data(a, b, default={'weight': 0})['weight'] + 1\n",
    "                G.add_edge(a, b, weight=w)\n",
    "\n",
    "    # Prune very weak edges if desired\n",
    "    if min_weight > 1:\n",
    "        G.remove_edges_from([(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_weight])\n",
    "\n",
    "    return G\n",
    "\n",
    "def draw_motif_graph(G: nx.Graph, out_path=None):\n",
    "    \"\"\"\n",
    "    Draws the motif graph. If there are nodes but no edges, shows isolated nodes with a note.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5.2, 5.2))\n",
    "    if G.number_of_nodes() == 0:\n",
    "        ax.text(0.5, 0.5, \"No motifs found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        if G.number_of_edges() == 0:\n",
    "            # Place nodes in a circle to avoid overlap, add a gentle note\n",
    "            pos = nx.circular_layout(G)\n",
    "            nx.draw_networkx_nodes(G, pos, node_color=\"#91b4ff\", node_size=900, ax=ax)\n",
    "            nx.draw_networkx_labels(G, pos, font_size=9, ax=ax)\n",
    "            ax.set_title(\"Motifs (no strong co-occurrence found)\")\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            pos = nx.spring_layout(G, seed=42)\n",
    "            widths = [1 + d['weight'] for *_u, _v, d in G.edges(data=True)]\n",
    "            nx.draw(\n",
    "                G, pos,\n",
    "                with_labels=True,\n",
    "                node_color=\"#91b4ff\",\n",
    "                node_size=900,\n",
    "                width=widths,\n",
    "                edge_color=\"#8aa0d6\",\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.axis(\"off\")\n",
    "    if out_path:\n",
    "        fig.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b498a27-d1a1-4e0a-8061-c88573b2d102",
   "metadata": {},
   "source": [
    "##  Full Multimodal + Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1d4a9b-9cab-4642-a7f6-ad08749f4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_from_text(user_text: str, project_tag: str = \"dream\", fast: bool = False):\n",
    "    clean_prompt, tox_score, softened = sanitize_prompt(user_text)\n",
    "    stamp = now_stamp()\n",
    "    base = OUT_DIR / f\"{project_tag}_{stamp}\"\n",
    "    base.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    art_prompt = (\n",
    "        \"Dreamlike surreal illustration, cinematic light, ethereal mood, 35mm film look. \"\n",
    "        f\"Abstract symbols: {clean_prompt[:500]}\"\n",
    "    )\n",
    "    img = text_to_image(\n",
    "        art_prompt,\n",
    "        steps=3 if fast else 4,\n",
    "        height=448 if fast else 512,\n",
    "        width=448 if fast else 512,\n",
    "    )\n",
    "    img_path = save_image(img, base.with_suffix(\".png\"))\n",
    "\n",
    "    audio_prompt = f\"Ambient calm pads, shimmering tones, inspired by: {clean_prompt[:150]}\"\n",
    "    wav_path = text_to_audio(audio_prompt, base.with_suffix(\".wav\"))\n",
    "\n",
    "    report = {\n",
    "        \"timestamp\": stamp,\n",
    "        \"input_text\": user_text,\n",
    "        \"used_prompt\": clean_prompt,\n",
    "        \"toxicity_score\": tox_score,\n",
    "        \"softened_prompt\": bool(softened),\n",
    "        \"image_model\": TXT2IMG_ID,\n",
    "        \"audio_engine\": AUDIO_ENGINE,\n",
    "        \"paths\": {\"image\": str(img_path), \"audio\": str(wav_path)}\n",
    "    }\n",
    "    with open(base.with_suffix(\".json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    return report\n",
    "\n",
    "def multimodal_with_extras(user_text: str, project_tag=\"dream\", mood=True, motifs=True):\n",
    "    report = multimodal_from_text(user_text, project_tag)\n",
    "    mb_path = None\n",
    "    g_path = None\n",
    "\n",
    "    if mood:\n",
    "        mb = text_to_moodboard(user_text)\n",
    "        mb_path = OUT_DIR / f\"{project_tag}_{now_stamp()}_moodboard.png\"\n",
    "        mb.save(mb_path)\n",
    "\n",
    "    if motifs:\n",
    "        G = build_motif_graph(user_text)\n",
    "        g_path = OUT_DIR / f\"{project_tag}_{now_stamp()}_motifs.png\"\n",
    "        draw_motif_graph(G, out_path=g_path)\n",
    "\n",
    "    report[\"extras\"] = {\n",
    "        \"moodboard\": str(mb_path) if mb_path else None,\n",
    "        \"motif_graph\": str(g_path) if g_path else None\n",
    "    }\n",
    "\n",
    "    # Persist the enriched report next to the image\n",
    "    json_path = Path(report[\"paths\"][\"image\"]).with_suffix(\".json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6d106-2d83-4706-bd7f-5efbe1d4bf4a",
   "metadata": {},
   "source": [
    "## üéôÔ∏è Optional: Speech-to-Text with Faster-Whisper\n",
    "\n",
    "This section adds microphone/upload audio input. We transcribe speech to text using **Faster-Whisper** on CPU for maximum compatibility (avoids the `unsupported device mps` error on Mac).  \n",
    "\n",
    "- Model size: `small` (you can try `tiny` for faster but less accurate).\n",
    "- Compute type: `int8` for speed.\n",
    "- Voice activity detection (VAD) enabled to trim silences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1adae68-babb-4ca7-8edb-c1d51742a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Loaded Faster-Whisper: small on cpu (int8)\n"
     ]
    }
   ],
   "source": [
    "# --- Faster-Whisper (ASR) ---\n",
    "import faster_whisper\n",
    "from typing import Optional\n",
    "\n",
    "# Use CPU to avoid \"unsupported device mps\" issues on Mac\n",
    "WHISPER_DEVICE = \"cpu\"           # or \"cuda\" if you have a working NVIDIA setup\n",
    "WHISPER_MODEL_SIZE = \"small\"     # try \"tiny\" for even faster, \"base\"/\"medium\" for more accuracy\n",
    "WHISPER_COMPUTE = \"int8\"         # good speed/accuracy tradeoff on CPU\n",
    "\n",
    "# Create one global model instance to avoid reloading on each click\n",
    "try:\n",
    "    ASR_MODEL = faster_whisper.WhisperModel(\n",
    "        WHISPER_MODEL_SIZE,\n",
    "        device=WHISPER_DEVICE,\n",
    "        compute_type=WHISPER_COMPUTE\n",
    "    )\n",
    "    print(f\"üé§ Loaded Faster-Whisper: {WHISPER_MODEL_SIZE} on {WHISPER_DEVICE} ({WHISPER_COMPUTE})\")\n",
    "except Exception as e:\n",
    "    ASR_MODEL = None\n",
    "    print(\"Faster-Whisper not available:\", e)\n",
    "\n",
    "def transcribe_audio(audio_path: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe an audio file path to text using Faster-Whisper.\n",
    "    Returns an empty string if ASR is unavailable or no audio given.\n",
    "    \"\"\"\n",
    "    if not audio_path or ASR_MODEL is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        segments, info = ASR_MODEL.transcribe(\n",
    "            audio_path,\n",
    "            vad_filter=True,\n",
    "            vad_parameters={\"min_silence_duration_ms\": 300},\n",
    "            beam_size=1,\n",
    "            best_of=1,\n",
    "            language=None  # auto-detect\n",
    "        )\n",
    "        text = \" \".join(seg.text.strip() for seg in segments).strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"ASR failed:\", e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90cb3bd6-4392-4729-af49-972eee294ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_with_audio(text_input, audio_input, prefer_audio, make_mood=True, make_motif=True, fast=False):\n",
    "    \"\"\"\n",
    "    - If 'prefer_audio' is True and a microphone/upload file is provided, we transcribe and use that.\n",
    "    - Otherwise we use the text box as usual.\n",
    "    \"\"\"\n",
    "    transcript = \"\"\n",
    "    if prefer_audio and audio_input:\n",
    "        transcript = transcribe_audio(audio_input)\n",
    "        if transcript:\n",
    "            text_input = transcript\n",
    "\n",
    "    text_input = (text_input or \"\").strip()\n",
    "    if not text_input:\n",
    "        return None, None, None, None, \"Please enter or record a dream.\", transcript\n",
    "\n",
    "    meta = multimodal_with_extras(text_input, mood=make_mood, motifs=make_motif)\n",
    "    return (\n",
    "        meta[\"paths\"][\"image\"],\n",
    "        meta[\"paths\"][\"audio\"],\n",
    "        meta[\"extras\"][\"moodboard\"],\n",
    "        meta[\"extras\"][\"motif_graph\"],\n",
    "        f\"Toxicity: {meta['toxicity_score']:.2f}\",\n",
    "        transcript\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fceea0-3b79-4576-8a26-beb4720b3112",
   "metadata": {},
   "source": [
    "# Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bf5e5-86d4-482b-a439-818a350ed7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ec56d5cb0e4a478dbc4a1c3ec770ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1490a2feb7c44cdf8eec90e31379877b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53983c5e71754608aa6c5eaf213f44f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2896f890d014f88ac9806b9337a0119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e9d11b13b14908b7d646eaba3863cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fc8def8ab04b558bc59d44dd9ffb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1a12f2cd9d40459e0f0fa76b6e42dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ba341e1b8544b184b94d63c0d6cbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f562a5fe5614b07b14544f9f79a3eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e4653046b04535ad5ab5b45ffcf06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff17add69b284c81b9fe7eac7b8d2dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba6d502d5fd44468d3dcd4e40e65228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65f19d808d04b579d4d1031eaf4bea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de80de2ac02b445b801c72e96d867b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradio UI with microphone/upload + transcript preview \n",
    "with gr.Blocks(title=\"DreamScape AI\") as demo:\n",
    "    gr.Markdown(\"## DreamScape AI ‚Äî Multimodal Dream Generator\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            tbox = gr.Textbox(label=\"Describe your dream (text input)\", lines=4)\n",
    "            aud = gr.Audio(\n",
    "                sources=[\"microphone\", \"upload\"],\n",
    "                type=\"filepath\",\n",
    "                label=\"Optional: record/upload your dream\"\n",
    "            )\n",
    "            prefer_audio = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"Prefer audio input if provided (use ASR transcript)\"\n",
    "            )\n",
    "            make_mood = gr.Checkbox(value=True, label=\"Generate moodboard\")\n",
    "            make_motif = gr.Checkbox(value=True, label=\"Generate motif graph\")\n",
    "            fast = gr.Checkbox(value=False, label=\"Fast mode (slightly lower quality, faster)\")\n",
    "            run = gr.Button(\"Generate\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            out_img = gr.Image(label=\"Generated Image\")\n",
    "            out_aud = gr.Audio(label=\"Generated Audio\")\n",
    "            out_mb = gr.Image(label=\"Moodboard\")\n",
    "            out_g = gr.Image(label=\"Motif Graph\")\n",
    "            out_t = gr.Textbox(label=\"Analysis\")\n",
    "            out_asr = gr.Textbox(label=\"Transcribed Text (if audio used)\")\n",
    "\n",
    "    run.click(\n",
    "        fn=run_all_with_audio,\n",
    "        inputs=[tbox, aud, prefer_audio, make_mood, make_motif, fast],\n",
    "        outputs=[out_img, out_aud, out_mb, out_g, out_t, out_asr]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
